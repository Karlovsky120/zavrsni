\documentclass[times, utf8, zavrsni, numeric]{fer}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{listings}

\lstset{
	basicstyle=\footnotesize,
	numbers=left,
	tabsize=2
}

\graphicspath{{./images/}}

\begin{document}

\thesisnumber{6651}

\title{Rendering of Voxelized Space with Vulkan Using Hardware Accelerated Ray Tracing}

\author{Ivan Karlović}

\maketitle

% Ispis stranice s napomenom o umetanju izvornika rada. Uklonite naredbu \izvornik ako želite izbaciti tu stranicu.
\izvornik

% Dodavanje zahvale ili prazne stranice. Ako ne želite dodati zahvalu, naredbu ostavite radi prazne stranice.
\zahvala{}

\tableofcontents

\chapter{Introduction}

Ever since OpenGL 1.0 was released in 1992., the computer hardware industry has been continuously improving on what GPUs are capable of. Today's graphics cards are boasting FP performance of over 10 TFLOPS, making them more than $10^{12}$ times faster than the ones initially released with OpenGL 1.0. While OpenGL has changed over the last 20 years (current version 4.6), it can no longer extract the full potential of the graphics cards built with modern architectures. This is why Vulkan was created, a new API designed from ground up for the modern GPU architectures. It is a more advanced API, leaving more control in the hands of the application, whereas in OpenGL a lot of operations were handled by the GPU drivers.

Modern graphics cards have reached another important milestone within the last few years. While realtime rendering has been done almost exclusively using rasterization, it has now become possible to render significant parts of the scene using ray tracing, such as shadow, reflection and global illumination. There are implementations today that even render the whole scene solely using ray tracing. A whole new pipeline has been created for modern graphic APIs (including Vulkan) that can utilize hardware to accelerate certain aspects of ray tracing, most notably ray triangle intersections. This paper will first explore how to efficiently represent voxelized space on the GPU and then render it using the new ray tracing pipeline.

\chapter{Used tools and technologies}
\section{C++}

\begin{center}
\includegraphics[width=0.1\textwidth]{cpp_logo.png}
\end{center}

C++ is a primarily object-oriented programming language. It was developed by Bjarne Stroustrup as an extension of the C language and was initially standardized by ISO in 1998, the current standard being C++17. Due to its speed and low-level memory management capabilities, it became the first choice for the development of 3D applications.

\section{Vulkan}

\begin{center}
\includegraphics[width=0.6\textwidth]{vulkan_logo.png}
\end{center}

Vulkan \cite{vulkan_spec} is a graphics API released on 26th of February 2016 by the Khronos consortium, an open industry consortium consisting of over 150 software and hardware companies. It's a cross-platform graphics and compute API which is constantly being worked and expanded upon. The current version, and the one used in this paper, is 1.2. While it’s capable of better utilizing the GPU resources, it’s not meant as a replacement for OpenGL which still works very well for most use cases. In Vulkan however, the application has a lot more control (and by extent, responsibility) over the application. A lot of features and functions that were handled and synchronized by the driver are now up to the application to deal with and control.

Vulkan is released as a C99 header file. Since its initial release, more than several different bindings for various languages have been created, including the ones for C++, C\#, Python, Java, Haskell and many others. There even exists a binding that allows for Direct3D 9 applications to run over Vulkan.

Along with Vulkan, a new standard for programmable shaders was developed, SPIR-V. It can be compiled from GLSL (and recently HLSL) source code ensuring more precise interpretation of the specification, addressing many issues that stemmed from GLSL and HLSL shaders behaving differently on different vendor hardware.

Throught this paper, Vulkan 1.2.141 used via C++ bindings.

\section{LunarG SDK}

\begin{center}
\includegraphics[width=0.2\textwidth]{lunarg_logo.png}
\end{center}

LunarG SDK is a Windows and Linux compatible Vulkan SDK which provides the various components need to develop a Vulkan application, including Vulkan loader, Vulkan layers, debugging tools, SPIR-V tools, Vulkan runtime installer, documentation samples and demos.

\section{GLFW}
GLFW is a Graphics Library FrameWork originally developed for OpenGL. It a simple API that today supports Vulkan that is used for creating windows and surfaces, as well as receiving inputs and events.

\section{Optix denoiser}
Optix denoiser \cite{nvidia_optix} is a part of the Nvidia Optix SDK that can be used standalone. It takes noisy images produced by ray tracing and outputs a denoised image. While there exists a more advanced variant that takes two additional images - one representing the albedo colour of each fragment and the other with the normal of each fragment, it isn't used in this paper.

\chapter{Representation of voxelized space using greedy meshing}
Voxelized space is represented by descreete elements at regular intervals, called voxels. Each voxel contains a single value, denoting whether it's opaque or transparent. Opaque voxels will be drawn, while transparent voxels will be treated as empty. To manage them easier, groups of 32x32x32 voxels are grouped into chunks. Both raster and ray tracing pipelines require triangles for input, so the question is how to transform chunks into a 3D mesh of triangles.

\section{The naive method}
The simplest way to generate a mesh from a chunks is to iterate through each and every voxel and check whether they are air or not. If the voxel is air, generate two triangles per voxel side, for a total of 12 triangles per voxel. The worst case scenario is a chunk filled with opaque voxels and in that case the algorithm creates 393,216 triangles. This method is fast thought, and each voxel relies only its own value.

\section{Optimizing the naive method}
A simple optimization can greatly reduce the number of triangles created by the naive method. Each neighbouring voxel is checked and a pair of triangles is created only if the neighbour is transparent. The worst case scenario from the previous example generates only 12,288 triangles. Note that this is no longer the worst case scenario for the optimized algorithm. That honour goes to a chunk filled 50\% with voxels spaced in checkerboard patter. In this case, number of triangles created is 196,608, the half of the naive method. This scenario will be the worst case for every other algorithm explored.

\section{Greedy meshing}
Unlike previous methods that analyse the chunk voxel by voxel, greedy meshing works by dividing the chunk into single voxel slices across each of the axes. Since each slice has two faces (eg. horizontal slice has the top and the bottom face), the actual number of slices is six.
For each slice, the algorithm looks for adjacent voxels and merges them into bigger rectangles. The algorithm creates only 12 triangles for a chunk filled with opaque voxels. One of the most important benefit of this algorithm which may not be apparent right away is in the fact that the triangle mesh is divided into slices. When a voxel is updated, instead of re-meshing the whole chunk, only six slices that lie on the surfaces of the updated voxel need to be re-meshed.

There are two optimizations made for this algorithm:
\begin{itemize}
	\item Chunks are padded with a single transparent block on each size. This removes the literal edge cases.
	\item Meshes are often stored as a pair: a list of (unique) vertices and a list of indices. Since in most models a single vertex is shared between multiple triangles, this often more than halves the mesh memory footprint.
	\item This implementation assumes there will be a lot of chunks created. This is why only a single vertex array which stores every possible vertex within a chunk is created. A chunk can be uniquely identified by only the index array.
\end{itemize}

Here is the algorithm that processes slices on the plane perpendicular to the x-axis. The inputs are a three-dimensional array representing a chunks (padded with air around it) and an integer which denotes which side of the slice is being processed, while the output is the index array. There are several things to note:
\begin{itemize}
	\item \texttt{meshed} is a temporary array that keeps track of already processed voxels, \texttt{updateMeshed} updates it every time a new quad is added to the list
	\item \texttt{pushRectangels} adds six new indices to the index array
	\item \texttt{getVertexIndex} gets the index of a particular vertex in the vertex array
	\item \texttt{P\_SIZE} is the padded size of a chunk (34 for a 32x32x32 chunk) while \texttt{CHUNK\_SIZE} is the actual size of the chunk (32 for a 32x32x32 chunk)
\end{itemize}

\begin{lstlisting}[language=c++][frame=single]
std::vector<uint32_t> xSlice(bool blocks[P_SIZE][P_SIZE][P_SIZE], int32_t side) {
	std::vector<uint32_t> slices;
	for (uint32_t i = 1; i < CHUNK_SIZE + 1; ++i) {
		bool meshed[CHUNK_SIZE][CHUNK_SIZE] = {};
		for (uint32_t j = 1; j < CHUNK_SIZE + 1; ++j) {
			for (uint32_t k = 1; k < CHUNK_SIZE + 1; ++k) {
				if (meshed[j - 1][k - 1] == false
					&& blocks[i][j][k] == true
					&& blocks[i + side][j][k] == false) {

					// Determine the height of the rectangle
					uint32_t dim1 = k - 1;
					do {
						++dim1;
					} while (meshed[j - 1][dim1 - 1] == false
						&& blocks[i][j][dim1] == true
						&& blocks[i + side][j][dim1] == false
						&& dim1 < CHUNK_SIZE + 1);

					// Determine the width of the rectangle
					uint32_t dim2 = j + 1;
					do {
						bool holeOrObstructionFound = false;
						for (uint32_t m = k; m < dim1; ++m) {
							if (blocks[i][dim2][m] == false
								|| blocks[i + side][dim2][m] == true) {
								holeOrObstructionFound = true;
								break;
							}
						}

						if (holeOrObstructionFound) {
							break;
						}

						++dim2;
					} while (dim2 < CHUNK_SIZE + 1);

					// Mark voxels as processed and update the slice with the quad
					updateMeshed(meshed, j, k, dim1, dim2);

					uint32_t v1 = getVertexIndex(i - 1, k - 1, j - 1);
					uint32_t v2 = getVertexIndex(i - 1, k - 1, dim2 - 1);
					uint32_t v3 = getVertexIndex(i - 1, dim1 - 1, j - 1);
					uint32_t v4 = getVertexIndex(i - 1, dim1 - 1, dim2 - 1);

					if (side == -1) {
						pushRectangle(slices, v1, v2, v3, v4);
					}
					else {
						pushRectangle(slices, v1, v3, v2, v4);
					}
				}
			}
		}
	}
	return slices;
}
\end{lstlisting}

The algorithms for planes perpendicular to other two axes are analogus.

\chapter{What is ray tracing and how it compares to rasterization}
The goal of both rasterization and ray tracing is to take a scene made out of primitives (most often, triangles) and transform it into a two dimensional image that can be shown on a screen. How they both go about doing that is vastly different.

\section{Ray tracing overview}
Various light sources in our environment launch fotons into their surroundings. Those photons bounce against various surfaces and some of the land in our eyes. Simulating every photon that a light source produces is neither viable or very efficient. Instead, in ray tracing, only the fotons that hit the eye of the observer are traced back to their source. For each pixel of the screen, a ray is traced from the eye of the observer through the pixel and into the scene. This ray is called the primary ray. The closest intersecition of the ray with the scene geometry is found, called a ray hit. From this point, several new rays can be formed:
\begin{itemize}
	\item Reflected ray
	\item Refracted ray
	\item Shadow rays
\end{itemize}

Reflected and refracted rays will be traced through the geometry of the scene, potentially creating new hits. The process will recursively repeat until a certain depth, or until the contribution the a ray becomes too insignificant. The shadow rays are shoot towards the light sources of the scene. If they don't intersect anything between the hit and the light source, the ray origin is considered lit by that light source. If the light source is a point light, a single ray is sufficient, generating hard shadows. For area lights, several points on the light are sampled and a ray is traced to each of them. This enables the generation of more realistic soft shadows.

Approach described above will generate an image that will look realistic, but with a lot of noise, especially if area lights are used. This can be dealt with in two ways:
\begin{itemize}
	\item Tracing more rays per pixel - instead of tracing a single ray trough the center of the pixel, multiple points on the pixel are sampled and a ray is traced through each. The final colour of the pixel is then the average result of each of the rays. In order for the image to converge, ie. for noise to be removed, the required number of rays per pixel s usually over a hundred, often several thousand. This is not viable for real time raytracing on current hardware, but will produce the results are closest to the ground truth.
	\item Using a denoising algorithm on the final image - the final image can be processed by an algorithm that removes the noise. The image can often be supplemented with addtional data, like the image of the scene that is uniformly lit (which can be a product of raster pipeline) or an image that instead of the colour holds the value of the normal of each pixel of the image. This method is much faster and denoising algorithms today will often produce results that are almost indistinguishable from the ground truth.
\end{itemize}

An algorithm called path tracing takes ray tracing a step further. Once a hit is found, instead of tracing only several specific rays, many rays will be generated going in random directions. This method is used for offline rendering, where the quality of the image is more important than the speed at which it can be produced.

\section{Rasterization overview}
The list of vertices is first taken through a vertex shader. Vertex shader will general take two forms of input. Per vertex attribues and constant data. Per vertex attributes can be vertex position, vertex normal or some other data specifict to a vertex while the constant data are often camera and light positions. Vertex shaders are fully 

Rasterization take an simplified but faster way to process the geometry. Each triangle is projected to the screen and turned into pixels. If the middle of the pixel is within the triangle, it will be considered a part of the triangle.

\chapter{Overview of Vulkan and its ray tracing extension}
While OpenGL is still continuously being updated, there is only so much that can be done for an API that is almost three decades old. This is why Vulkan was created, designed from the ground up with the modern architecture of graphics cards in mind. It's a cross-platform graphics and compute API that can run on a variety of operating systems and which many devices can support. Due to all of this, Vulkan is a complex and extremely verbose API. While on the surface it might look like it's a lot more complex than OpenGL, the difference I much smaller than it appears to be on the first glance. A lot of complexity (and verbosity) comes from the fact that a lot of responsibility (and by extension, control) has been shifted from the driver to the application.

As a result, a hello triangle program that might have taken 20 or so lines in OpenGL 1.0 now takes over 900 lines of code in Vulkan \cite{vulkan_tutorial}.

There are several key points that make Vulkan more performant and more suitable for modern systems than OpenGL:
\begin{itemize}
	\item Low API overhead. Instead of issuing commands to the device one by one, multiple commands are recorded to command buffers which are then submitted to the GPU to be executed.
	\item API makes no guarantees on order of execution of commands recorded to the submitted command buffer. The application needs to use pipeline and memory barriers to create dependencies between commands. In a well-written application, this means that commands will only be blocked by the commands that they depend on. This extends to other mechanism as well, often explicit dependencies need to be stated, giving the driver the freedom to arrange the workload in the most optimal way, while respecting those dependecies.
	\item API has been written with multicore CPUs in mind. While it's not possible to concurrently record commands to the same command buffer from multiple threads, it is possible to create and (concurrently) record commands to secondary command buffers that can then be tied together in a primary command buffer.
	\item API controls memory allocations. In OpenGL, all device memory allocations have been managed by the driver, whereas in Vulkan all memory allocations need to be explicitly managed by the application.
	\item Most objects in the API are immutable. While this makes the API a bit more cumbersome to use, the driver can use this guarantee to make optimizations that weren't possible in OpenGL.
	\item Better debugging support. Vulkan introduces the concept of validation layers - layers that can be placed between the application and the driver to ensure that calls to the API are valid and well-formed.
\end{itemize}

\section{Vulkan model overview}
\subsection{Instance and device}
The first object to be created when using Vulkan is an instance (\texttt{vk::Instance}). The application can specify which version of the API will be used, as well as enable various validation layers. The instance is the closest thing in Vulkan to OpenGLs global state. Once the instance is created, various physical devices can be enumerated (\texttt{vk::PhysicalDevice}), each representing specific Vulkan-compatible devices connected to the system. They can be queried for their capabilities and various extension support. Next step is to create a logical device (\texttt{vk::Device}) based on one of the physical devices. From this point on, the logical device facilitates most of applications communication with the API.

\subsection{Queues and command buffers}
Most of communication between the application and the device is done with command pools (\texttt{vk::CommandPool}), command buffers (\texttt{vk::CommandBuffer}) and queues (\texttt{vk::Queue}). First, a command pool is created, connected to one of the queues available from the physical device. It's possible to then allocate one or more command buffers from this pool. Once the commands are recorded to the command buffer, it can be submitted to the queue for the device to execute it.

\subsection{Synchronization}
\subsubsection{Synchronization within buffers}
All of the commands in a command buffer are executed asynchronously. Pipeline barriers act as execution barriers between various commands, ensuring various commands are executed after described pipeline stages. Each pipeline barrier can hold a reference to one or more image and buffer memory barriers (\texttt{vk::ImageMemoryBarrier} and \texttt{vk::BufferMemoryBarrier}), determining the order of the operations over those resources (eg. preventing read operations on a buffer or an image before all writes have been completed).

\subsubsection{Synchronization between buffers}
Submitting one command buffer before the other does not guarantee it will be executed first. On each command buffer submission, it's possible to supply one or more binary semaphores (\texttt{vk::Semaphore}) that the command buffer will wait on to be signalled before executing, or semaphores that will be signalled once the command buffer has completed its execution. Fences (\texttt{vk::Fence}) are used similarly, but they enable command buffer synchronization with the application (eg. waiting for an image to be rendered before attempting to present it on screen). In Vulkan 1.2, timeline semaphores have been introduced with the \texttt{VK\_KHR\_TIMELINE\_SEMAPHORE} extension. It enables more complex dependencies to be expressed with a single semaphore holding several points in time instead of being forced to use a binary semaphore between each of the steps.

\subsection{Memory management overview}
Every Vulkan object created by the application will have its own handle. Once an object is created, the application has to keep track of it and destroy it once it's no longer needed. Most Vulkan objects don't need additional memory other than its own metadata. The two exceptions are pooled resources and application managed resources.

\subsubsection{Pooled resources}
Two examples of pooled resources are descriptor sets and command buffers. First a pool object (\texttt{vk::DescriptorPool} or \texttt{vk::CommandPool}) needs to be created from which desired objects are then allocated. The memory of these pools is managed by the device driver.

\subsubsection{Application managed resources}
Two most notable entries of this category are buffers (\texttt{vk::Buffer}) and Images (\texttt{vk::Image}). Once a handle to one of these resources is created, the application must query for the memory requirements of the object and then allocate device memory (\texttt{DeviceMemory}) with suitable properties. Finally, this memory must then be bound to the object. It's recommended to allocate a single device memory and bind several objects to various offsets, especially since its possible to have only 4096 allocated device memory at one time (on Windows at least, Linux doesn't suffer from such limitations). Since most API calls that take buffers as arguments will often take both offset and size, it's possible to use a single buffer for multiple purposes. Similar can be done with images and image views (\texttt{vk::ImageView}), where image view can enable access to only a part of the image.

\subsection{The swapchain and the framebuffers}
The swapchain (\texttt{vk::SwapchainKHR}) represents a set of images that can be presented to the screen. This is a rare example where driver handles the memory allocations for an image resource. These images can be presented to a surface (\texttt{vk::SurfaceKHR}). \texttt{vk::SurfaceKHR} is an abstraction over several possible surface types, ranging from Win32 surface, all the way to Android surface. Note that both of these objects are a part of \texttt{VK\_KHR\_SWAPCHAIN} extension. Since Vulkan-capable devices can be compute only, the ability to present to a surface is not a part of the core specification but is included as an extension which support is optional. When swapchain is used with the raster pipeline, each image needs to be bound to a framebuffer (\texttt{vk::Framebuffer}). It's also possible to add more than one image - unlike in OpenGL, the image for storing the data for the depth and stencil tests needs to be explicitly created and bound to a framebuffer.  

\subsection{The pipeline and shaders}
There are three types of pipelines (\texttt{vk::Pipeline}) in Vulkan:
\begin{itemize}
	\item{Compute pipeline}
	\item{Graphics pipeline}
	\item{Ray tracing pipeline}
\end{itemize}

Pipelines are objects that define which shaders will participate in rendering and, depending on their type, can define certain aspects of rendering. For example, settings on how and if triangles are culled are set in the raster pipeline. For the most part pipelines in Vulkan are immutable, so in order to change almost anthing in the pipeline (like swapping out a shader), a whole new pipeline needs to be constructed. Since pipeline construction is one of more time intensive workloads in Vulkan, pipeline caches (\texttt{vk::PipelineCache}) have been created that can store parts of previously constructed pipelines which are then reused when constructing new, similar pipelines. It's also advised to construct pipelines asynchronusly and ahead of time.
Shaders are encapsulated in shader modules (\texttt{vk::ShaderModule}) which take a SPIR-V binary format. It's possible to compile GLSL (and more recently HLSL) source code to SPIR-V binary format using glslangValidator, but it's also possible to compile them at runtime using the shaderc library. Both glslangValidator and shaderc are distributed with LunarG SDK.

\subsection{Descriptor sets and push constants}
Descriptor sets (\texttt{vk::DescriptorSet}) are the interfaces between the shaders of the pipeline and the data that needs to be accessible from those shaders. Each descriptor set has one or more bindings. It's possible to bind various resources to a descriptor set, like samplers, images, buffer and acceleration strucutres (if the ray tracing extension is used). This is how things like materials, textures, lighting data and other scene data is passed to the shaders.
Push constants is a small buffer (often only 256 bytes) that can be written to directly. This is often used for small amount of data that is constant over the length of a frame (such as view and projection matrices). It's can also be used for data pertaining to the following draw call and in that case it's updated before each new draw call.

\subsubsection{The render pass}

\section{VK\_KHR\_ray\_tracing extension}
On 27th of November 2018., Khronos released Vulkan revision 1.1.92.0 which introduced \texttt{VK\_NV\_ray\_tracing} extension, which enabled the utilization of specialized hardware of Nvidia Turing GPUs for real-time raytracing. Roughly a year and a half later, on 17th of March 2020. Khronos has released \texttt{VK\_KHR\_ray\_tracing} extension that has several advantages over the NV variant:
\begin{itemize}
	\item{The extension is vendor neutral, making it more likely for vendors other than Nvidia to support it.}
	\item{Extension offers more flexibility in building acceleration structures.}
\end{itemize}

At the time of writing of this paper, the extension is still in beta and feedback is being taken from the develpers using it.

The extension itself introduces a several new concepts to Vulkan:
\begin{itemize}
	\item{Acceleration structures}
	\item{Ray tracing shaders}
	\item{Shader binding table}
	\item{Ray tracing pipeline}
\end{itemize}

\subsection{Acceleration structures}
Acceleration structures (\texttt{vk::AccelerationStructure}) are objects used to accelerate ray tracing. Instead of testing each ray for intersection with each and every triangle in the scene, a BVH is constructed that reduces complexity of the search from O(n) to O(logn). There are two types of acceleration structures in the API. Bottom acceleration structures (BLAS) can hold one or more geometries. Each geometry consists of a list of vertices (which can be indexed). It's possible to attach a transformation to the BLAS. Top acceleration structures (TLAS) reference BLASes. Each instance of BLAS is given a unique identifier (even if it references the same BLAS) which is later available in the shader.

Acceleration structures are usually built on the GPU, but with the new extension, it's now possible to build them on the CPU and upload them to the GPU later. In cases where GPU is under a heavy load while the CPU has one or more cores idle, it's now possible to have acceleration structure be build using those idle cores, balancing the workload between the GPU and the CPU.

The actual process of building the acceleration structure is handled by the driver. It's possible to instruct the driver to build the structure with ray tracing speed or speed of the build itself being prioritized. Acceleration structures, if so specified during their creation, can also be updated, which is faster than rebuilding it. Not all acceleration structures are suitable for updates - where tree with branches swaying on the wind would be good acceleration structure to update, one depicting an object which is exploding would not.

\subsection{Ray tracing shaders}
This extension introduces several new shader types:
\begin{itemize}
	\item{Ray generation shader}
	\item{Any hit shader}
	\item{Closest hit shader}
	\item{Miss shader}
	\item{Intersection shader}
	\item{Callable shader}
\end{itemize}

Since the shaders themselves are not a part of Vulkan API, they are introduces as a SPIR-V extension \texttt{SPV\_KHR\_ray\_tracing}

\subsubsection{Ray generation shader}
Ray tracing pipeline is entered similarly to a compute pipeline. A workload is submitted in three dimensions. First two dimensions are often used as screen dimensions in pixels, while the third one is usually set to 1, or can be used as a number of samples per pixel. For each 3D coordinate, a ray generation shader is invoked. The most often use of this shader is to calculate the trajectory of the ray passing through the designated pixel, trace a ray (\texttt{traceRayEXT} function) and write the result in an image. It's possible to define the segment of the ray that the tests will be perfomed on, serving a similar purpose to far and near plane of the scene frustum.

\subsubsection{Any hit shader}
Once traceRayEXT is invoked, the hardware tests for intersections between the triangles of the TLAS given and the ray itself. For each triangle hit, this shader is invoked. It's advised to keep these type of shaders short and fast, since it gets invoked many times per ray. This shader is optional and doesn't need to be implemented.

\subsubsection{Closest hit shader}
After the ray is tested agains all of the TLAS, this shader is invoked over the closest intersection found.

\subsubsection{Miss shader}
If no intersection is found between the TLAS and the ray, this shader is invoked. This can be used to sample a cube map in order to render a skybox around the camera, or can be used to signal that there is no obstruction between the ray origin and the ray target, which is very useful for shadow testing.

\subsubsection{Intersection shader}
While the geometry will usually be defined as a set of triangles, it's possible to define it as any object, as long as it can be fit within an axil aligned bounding box (AABB). If that is the case, the testing will be done against the AABBs and on hit the intersection shader will be invoked, making it possible to calculate if and how the ray hits the bounded object. This shader needs to be defined only if TLAS contains a BLAS whose geometry type is AABB.

\subsubsection{Callable shader}
Callable shaders are shaders that can be called from other programmable shaders. Due to paralellism of the GPU, the driver might be able to better optimize the calls than if simple functions were invoked.

\subsection{Shader binding table}
With standard rasterization, various objects with different properties can be rendered one after another, swaping the shaders in and out as needed. When it comes to ray tracing, the ray can hit any object in the scene, requiring all shader to be available at the same time. Shader binding table holds the handles to every shader that could be invoked during the ray tracing process. The handles are divided into groups. Ray generation shader and miss shaders are in their own separate groups, while any hit shaders and closest hit shaders from a hit group. Shader binding table is referenced in two contexts - first when initially invoking the ray generation shaders from within the command buffer, second from within shaders for recursive calls.

\subsection{Ray tracing pipeline}
Ray tracing pipeline is quite a bit simpler than it's raster counter part since there isn't much to setup. It's main use is to bind the various shaders and hit groups in a single object. 

\chapter{Implementation of a simple ray traced application}
The goal is to create an application that use the ray tracing pipeline and components to render a voxelized space to the screen. The application has been developed with several switches that toggle certain aspects of the application on or off. Two of the most important ones are \texttt{RTX} and \testtt{OPTIX_DENOISER}.

\chapter{Conclusion}
Ray tracing simplifies a lot of concepts which, when using rasterization, need to be simulated by complex algorithms. When properly implemented, ray tracing produces soft shadows, accurate reflections and detailed global illumination, and does so inherently.

Its main drawback is its prohibitive cost - while GPUs today are technically capable of running the algorithm in real-time, it just isn't fast enough to do everything we'd like naively, requiring some kind of compromise:
\begin{itemize}
	\item limiting ray tracing to only certain aspects of the scene (only shadows, only reflections or only global illumination), while using rasterization for the rest
	\item ray tracing with a low number of samples per pixel, resulting in noisy images that needs to be denoised
	\item ray tracing at lower resolutions using one of the upscaling techniques to get the full resolution image
	\item accumulating lighting and other over several frames, creating a higher quality renders, but with visible lighting latency and artefacts
\end{itemize}

Methods enumerated above are only a few possible optimizations and many implementations often use more than one of them. Especially interesting is rendering at a lower resolution and upscaling - Nvidia has been developing a deep learning powered technique called DLSS (Deep Learning Super Sampling) which runs in constant time, but produces very good results.

However drastic the advancements in raytracing have been in few recent years, the (realtime) rendering industry has been using rasterization ever since GPUs were invented, developing the field for more than 30 years. With that in mind, it is abundantly clear that ray tracing will not be replacing rasterization in any foreseeable future. It will be used more often though, especially as more powerful hardware becomes available.

\bibliography{literatura}
\bibliographystyle{fer}

\newpage
\vspace*{\fill}
\thispagestyle{empty}
\begin{center}
	{\bf Interaktivan prikaz vokseliziranog prostora s Vulkanom uz sklopovski ubrzano praćenje zrake}
\end{center}
\hspace*{\fill} {\bf Sa\v{z}etak} \hspace*{\fill} \par
\vspace*{25pt}

U ovom radu razrađen je prikaz konačnog vokseliziranog prostora upotrebom sklopovski ubrzanog algoritma praćenja zrake. Uspoređena je standardna rasterizacija s algoritmom praćenja zraka te su opisani prednosti i mane jednog nad drugim. Posebna pažnja dana je načinu na koji Vulkan izlaže sklopovski ubrzano praćenje zrake kroz ekstenziju VK\_KHR\_RAY\_TRACING. Konačno, dan je malen Vulkan primjer koji demonstrira osnovnu implementaciju algoritma praćenja zrake.

\kljucnerijeci{algoritam praćenja zrake, Vulkan, voksel, sklopovsko ubrzanje}

\engtitle{Rendering of Voxelized Space with Vulkan Using Hardware Accelerated Ray Tracing}
\begin{abstract}
This paper explores the real-time representation of finite voxelized space using hardware-accelerated ray tracing. It compares standard rasterization to ray tracing and outlines the benefits and drawbacks of one over the other. It's explored how Vulkan exposes hardware ray tracing capabilities through its  VK\_KHR\_RAY\_TRACING extension. Finally, a small Vulkan example is given that shows a basic implementation of the ray tracing algorithm.

\keywords{ray tracing, Vulkan, voxel, hardware acceleration}
\end{abstract}

\end{document}